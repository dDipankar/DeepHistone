{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "\n",
    "import six\n",
    "import csv\n",
    "from six.moves import range\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, average_precision_score\n",
    "from keras.preprocessing import sequence\n",
    "from keras.optimizers import RMSprop,Adam, SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import  Dropout, Activation, Flatten\n",
    "from keras.regularizers import l1,l2,l1_l2\n",
    "from keras.constraints import maxnorm\n",
    "#from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dense, LSTM, Bidirectional\n",
    "#from keras.utils import plot_model\n",
    "#from keras.utils.layer_utils import print_layer_shapes\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1369)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5filename = \"histonemodKmer_resample_ncl_K562.h5\"\n",
    "h5file = h5.File(h5filename,'r')\n",
    "input_features = h5file['input/H3K9ac_kmer_2000']\n",
    "output_H3K9ac = h5file['output/H3K9ac_2000']\n",
    "input_features = np.array(input_features,dtype='int8')\n",
    "output_H3K9ac = np.array(output_H3K9ac, dtype='int8')\n",
    "output_H3K9ac_reshape = output_H3K9ac.reshape(len(output_H3K9ac),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "[    0     4     6 ..., 23946 23950 23953]\n",
      "(23954, 2081)\n",
      "(8770, 1)\n",
      "(15184, 1)\n",
      "(8770, 2081)\n",
      "(15184, 2081)\n",
      "[[27  8 10 ...,  4  1  5]\n",
      " [16  6  3 ...,  4  1  1]\n",
      " [ 4  3  0 ...,  4  2  2]\n",
      " ..., \n",
      " [20  7  8 ...,  9  3  4]\n",
      " [37  4  7 ...,  4  0  9]\n",
      " [ 0  1  0 ...,  3  1  0]]\n"
     ]
    }
   ],
   "source": [
    "#combine the label with input dna\n",
    "input_features_label = np.concatenate((input_features,output_H3K9ac_reshape), axis=1)\n",
    "H3K9ac_df = pd.DataFrame(output_H3K9ac)\n",
    "pos_label= H3K9ac_df.loc[H3K9ac_df.iloc[:,0]==1]\n",
    "pos_label_ix = np.array(pos_label.index)\n",
    "neg_label = H3K9ac_df.loc[H3K9ac_df.iloc[:,0]==0]\n",
    "neg_label_ix = np.array(neg_label.index)\n",
    "pos_sam_H3K9ac = input_features_label[pos_label_ix,:]\n",
    "neg_sam_H3K9ac = input_features_label[neg_label_ix,:]\n",
    "print('here')\n",
    "print(pos_label_ix)\n",
    "print(input_features_label.shape)\n",
    "print(pos_label.shape)\n",
    "print(neg_label.shape)\n",
    "print(pos_sam_H3K9ac.shape)\n",
    "print(neg_sam_H3K9ac.shape)\n",
    "print(input_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10628  6139]\n",
      "(16767, 2080)\n",
      "(16767,)\n"
     ]
    }
   ],
   "source": [
    "#train\n",
    "train_neg_sample = int(neg_sam_H3K9ac.shape[0] * 0.7)\n",
    "train_pos_sample = int(pos_sam_H3K9ac.shape[0] * 0.7)\n",
    "train_neg_H3K9ac = neg_sam_H3K9ac[0:train_neg_sample,:]\n",
    "train_pos_H3K9ac = pos_sam_H3K9ac[0:train_pos_sample,:]\n",
    "train_neg_pos_H3K9ac = np.concatenate((train_neg_H3K9ac, train_pos_H3K9ac),axis = 0)\n",
    "np.random.shuffle(train_neg_pos_H3K9ac)\n",
    "X_train_H3K9ac = train_neg_pos_H3K9ac[:,0:2080]\n",
    "Y_train_H3K9ac = train_neg_pos_H3K9ac[:,2080]\n",
    "frq = np.bincount(Y_train_H3K9ac)\n",
    "print(frq)\n",
    "print(X_train_H3K9ac.shape)\n",
    "print(Y_train_H3K9ac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1518  877]\n",
      "(2395, 2080)\n",
      "(2395,)\n"
     ]
    }
   ],
   "source": [
    "#val\n",
    "val_neg_sample = train_neg_sample + int(neg_sam_H3K9ac.shape[0] * 0.1)\n",
    "val_pos_sample = train_pos_sample + int(pos_sam_H3K9ac.shape[0] * 0.1)\n",
    "val_neg_H3K9ac = neg_sam_H3K9ac[train_neg_sample:val_neg_sample,:]\n",
    "val_pos_H3K9ac = pos_sam_H3K9ac [train_pos_sample:val_pos_sample,:]\n",
    "val_neg_pos_H3K9ac = np.concatenate((val_neg_H3K9ac, val_pos_H3K9ac),axis = 0)\n",
    "np.random.shuffle(val_neg_pos_H3K9ac)\n",
    "X_val_H3K9ac = val_neg_pos_H3K9ac[:,0:2080]\n",
    "Y_val_H3K9ac = val_neg_pos_H3K9ac[:,2080]\n",
    "frq = np.bincount(Y_val_H3K9ac)\n",
    "print(frq)\n",
    "print(X_val_H3K9ac.shape)\n",
    "print(Y_val_H3K9ac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3038 1754]\n",
      "(4792, 2080)\n",
      "(4792,)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "test_neg_H3K9ac = neg_sam_H3K9ac[val_neg_sample:,:]\n",
    "test_pos_H3K9ac = pos_sam_H3K9ac [val_pos_sample:,:]\n",
    "test_neg_pos_H3K9ac = np.concatenate((test_neg_H3K9ac, test_pos_H3K9ac),axis = 0)\n",
    "np.random.shuffle(test_neg_pos_H3K9ac)\n",
    "X_test_H3K9ac = test_neg_pos_H3K9ac[:,0:2080]\n",
    "Y_test_H3K9ac = test_neg_pos_H3K9ac[:,2080]\n",
    "frq = np.bincount(Y_test_H3K9ac)\n",
    "print(frq)\n",
    "print(X_test_H3K9ac.shape)\n",
    "print(Y_test_H3K9ac.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"H3K9ac_ncl_test\": shape (5090,), type \"|i1\">"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               1065472   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 180)               92340     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 70)                12670     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 71        \n",
      "=================================================================\n",
      "Total params: 1,170,553\n",
      "Trainable params: 1,170,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " model = Sequential()\n",
    " #model.add(Conv1D(activation=\"relu\", input_shape=(2080, 1), padding=\"valid\", strides=1, filters=256, kernel_size=11, kernel_initializer='glorot_uniform',kernel_regularizer=l2(0.001)))\n",
    " #model.add(MaxPooling1D(pool_size=4))\n",
    " #model.add(Dropout(0.6))\n",
    " #model.add(Conv1D(activation=\"relu\", padding=\"valid\", strides=1, filters=640, kernel_size=3, kernel_initializer='glorot_uniform', kernel_regularizer=l2(0.001)))\n",
    " #model.add(MaxPooling1D(pool_size=2))\n",
    " #model.add(Dropout(0.5))\n",
    " #model.add(Flatten())\n",
    " #model.summary()\n",
    " model.add(Dense(units=512, input_dim=2080, activation=\"relu\", kernel_initializer='glorot_uniform'))\n",
    " model.add(Dropout(0.5))\n",
    " #model.add(Dense(units=512, input_dim=512,  activation=\"relu\", kernel_initializer='glorot_uniform',kernel_regularizer=l2(0.001)))\n",
    " #model.add(Dropout(0.5))\n",
    " model.add(Dense(units=180, activation=\"relu\",kernel_initializer='glorot_uniform'))\n",
    " model.add(Dropout(0.5))\n",
    " model.add(Dense(units=70, activation=\"relu\",kernel_initializer='glorot_uniform'))\n",
    " model.add(Dense(units=1, activation=\"sigmoid\"))\n",
    " model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running at most 60 epochs\n",
      "Train on 16767 samples, validate on 2395 samples\n",
      "Epoch 1/50\n",
      "15232/16767 [==========================>...] - ETA: 0s - loss: 0.6956 - acc: 0.7185\n",
      "Epoch 00001: val_loss improved from inf to 0.41348, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 1s 71us/step - loss: 0.6751 - acc: 0.7246 - val_loss: 0.4135 - val_acc: 0.8489\n",
      "Epoch 2/50\n",
      "15744/16767 [===========================>..] - ETA: 0s - loss: 0.4284 - acc: 0.8183\n",
      "Epoch 00002: val_loss improved from 0.41348 to 0.34354, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 0.4248 - acc: 0.8195 - val_loss: 0.3435 - val_acc: 0.8693\n",
      "Epoch 3/50\n",
      "16128/16767 [===========================>..] - ETA: 0s - loss: 0.3790 - acc: 0.8432\n",
      "Epoch 00003: val_loss improved from 0.34354 to 0.31389, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 0s 28us/step - loss: 0.3797 - acc: 0.8433 - val_loss: 0.3139 - val_acc: 0.8768\n",
      "Epoch 4/50\n",
      "15872/16767 [===========================>..] - ETA: 0s - loss: 0.3518 - acc: 0.8588\n",
      "Epoch 00004: val_loss improved from 0.31389 to 0.29253, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 0.3526 - acc: 0.8578 - val_loss: 0.2925 - val_acc: 0.8889\n",
      "Epoch 5/50\n",
      "16640/16767 [============================>.] - ETA: 0s - loss: 0.3277 - acc: 0.8647\n",
      "Epoch 00005: val_loss improved from 0.29253 to 0.28790, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 0.3275 - acc: 0.8649 - val_loss: 0.2879 - val_acc: 0.8864\n",
      "Epoch 6/50\n",
      "16128/16767 [===========================>..] - ETA: 0s - loss: 0.3155 - acc: 0.8725\n",
      "Epoch 00006: val_loss improved from 0.28790 to 0.27204, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 0.3152 - acc: 0.8725 - val_loss: 0.2720 - val_acc: 0.8939\n",
      "Epoch 7/50\n",
      "16384/16767 [============================>.] - ETA: 0s - loss: 0.3047 - acc: 0.8792\n",
      "Epoch 00007: val_loss improved from 0.27204 to 0.26945, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 0.3052 - acc: 0.8787 - val_loss: 0.2694 - val_acc: 0.8885\n",
      "Epoch 8/50\n",
      "15744/16767 [===========================>..] - ETA: 0s - loss: 0.2879 - acc: 0.8853\n",
      "Epoch 00008: val_loss improved from 0.26945 to 0.26014, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 0.2874 - acc: 0.8854 - val_loss: 0.2601 - val_acc: 0.9052\n",
      "Epoch 9/50\n",
      "15232/16767 [==========================>...] - ETA: 0s - loss: 0.2874 - acc: 0.8841\n",
      "Epoch 00009: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 0.2876 - acc: 0.8842 - val_loss: 0.2645 - val_acc: 0.8998\n",
      "Epoch 10/50\n",
      "15872/16767 [===========================>..] - ETA: 0s - loss: 0.2807 - acc: 0.8907\n",
      "Epoch 00010: val_loss improved from 0.26014 to 0.25444, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 0.2790 - acc: 0.8916 - val_loss: 0.2544 - val_acc: 0.9027\n",
      "Epoch 11/50\n",
      "15488/16767 [==========================>...] - ETA: 0s - loss: 0.2739 - acc: 0.8935\n",
      "Epoch 00011: val_loss improved from 0.25444 to 0.25241, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 0s 30us/step - loss: 0.2714 - acc: 0.8941 - val_loss: 0.2524 - val_acc: 0.9048\n",
      "Epoch 12/50\n",
      "15744/16767 [===========================>..] - ETA: 0s - loss: 0.2663 - acc: 0.8984\n",
      "Epoch 00012: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 0.2683 - acc: 0.8977 - val_loss: 0.2551 - val_acc: 0.9073\n",
      "Epoch 13/50\n",
      "16512/16767 [============================>.] - ETA: 0s - loss: 0.2570 - acc: 0.8981\n",
      "Epoch 00013: val_loss did not improve\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 0.2563 - acc: 0.8984 - val_loss: 0.2541 - val_acc: 0.9031\n",
      "Epoch 14/50\n",
      "14848/16767 [=========================>....] - ETA: 0s - loss: 0.2545 - acc: 0.9017\n",
      "Epoch 00014: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 29us/step - loss: 0.2548 - acc: 0.9020 - val_loss: 0.2565 - val_acc: 0.9027\n",
      "Epoch 15/50\n",
      "15744/16767 [===========================>..] - ETA: 0s - loss: 0.2491 - acc: 0.9014\n",
      "Epoch 00015: val_loss improved from 0.25241 to 0.25223, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 0.2489 - acc: 0.9018 - val_loss: 0.2522 - val_acc: 0.9035\n",
      "Epoch 16/50\n",
      "15872/16767 [===========================>..] - ETA: 0s - loss: 0.2411 - acc: 0.9056\n",
      "Epoch 00016: val_loss improved from 0.25223 to 0.24517, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 1s 33us/step - loss: 0.2407 - acc: 0.9055 - val_loss: 0.2452 - val_acc: 0.9123\n",
      "Epoch 17/50\n",
      "16512/16767 [============================>.] - ETA: 0s - loss: 0.2380 - acc: 0.9075\n",
      "Epoch 00017: val_loss did not improve\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 0.2379 - acc: 0.9074 - val_loss: 0.2519 - val_acc: 0.9040\n",
      "Epoch 18/50\n",
      "15616/16767 [==========================>...] - ETA: 0s - loss: 0.2275 - acc: 0.9146\n",
      "Epoch 00018: val_loss improved from 0.24517 to 0.24265, saving model to HistoneMark_H3K9ac.hdf5\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 0.2282 - acc: 0.9143 - val_loss: 0.2427 - val_acc: 0.9127\n",
      "Epoch 19/50\n",
      "15744/16767 [===========================>..] - ETA: 0s - loss: 0.2209 - acc: 0.9139\n",
      "Epoch 00019: val_loss did not improve\n",
      "16767/16767 [==============================] - 1s 31us/step - loss: 0.2233 - acc: 0.9126 - val_loss: 0.2452 - val_acc: 0.9115\n",
      "Epoch 20/50\n",
      "15872/16767 [===========================>..] - ETA: 0s - loss: 0.2187 - acc: 0.9160\n",
      "Epoch 00020: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 28us/step - loss: 0.2195 - acc: 0.9157 - val_loss: 0.2480 - val_acc: 0.9073\n",
      "Epoch 21/50\n",
      "16000/16767 [===========================>..] - ETA: 0s - loss: 0.2105 - acc: 0.9179\n",
      "Epoch 00021: val_loss did not improve\n",
      "16767/16767 [==============================] - 1s 30us/step - loss: 0.2116 - acc: 0.9173 - val_loss: 0.2490 - val_acc: 0.9094\n",
      "Epoch 22/50\n",
      "16000/16767 [===========================>..] - ETA: 0s - loss: 0.2082 - acc: 0.9208\n",
      "Epoch 00022: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 27us/step - loss: 0.2088 - acc: 0.9201 - val_loss: 0.2485 - val_acc: 0.9098\n",
      "Epoch 23/50\n",
      "16000/16767 [===========================>..] - ETA: 0s - loss: 0.2069 - acc: 0.9174\n",
      "Epoch 00023: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 28us/step - loss: 0.2077 - acc: 0.9171 - val_loss: 0.2665 - val_acc: 0.8990\n",
      "Epoch 24/50\n",
      "15360/16767 [==========================>...] - ETA: 0s - loss: 0.1994 - acc: 0.9229\n",
      "Epoch 00024: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 28us/step - loss: 0.2003 - acc: 0.9220 - val_loss: 0.2489 - val_acc: 0.9073\n",
      "Epoch 25/50\n",
      "16128/16767 [===========================>..] - ETA: 0s - loss: 0.1947 - acc: 0.9225\n",
      "Epoch 00025: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 27us/step - loss: 0.1946 - acc: 0.9231 - val_loss: 0.2443 - val_acc: 0.9132\n",
      "Epoch 26/50\n",
      "15360/16767 [==========================>...] - ETA: 0s - loss: 0.1889 - acc: 0.9251\n",
      "Epoch 00026: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 28us/step - loss: 0.1887 - acc: 0.9256 - val_loss: 0.2496 - val_acc: 0.9098\n",
      "Epoch 27/50\n",
      "15232/16767 [==========================>...] - ETA: 0s - loss: 0.1796 - acc: 0.9293\n",
      "Epoch 00027: val_loss did not improve\n",
      "16767/16767 [==============================] - 1s 32us/step - loss: 0.1791 - acc: 0.9297 - val_loss: 0.2528 - val_acc: 0.9086\n",
      "Epoch 28/50\n",
      "15616/16767 [==========================>...] - ETA: 0s - loss: 0.1719 - acc: 0.9340\n",
      "Epoch 00028: val_loss did not improve\n",
      "16767/16767 [==============================] - 0s 28us/step - loss: 0.1719 - acc: 0.9335 - val_loss: 0.2510 - val_acc: 0.9123\n",
      "Epoch 00028: early stopping\n",
      "4792/4792 [==============================] - 0s 30us/step\n",
      "[0.28427479317868892, 0.89941569282136891]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               1065472   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 180)               92340     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 180)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 70)                12670     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 71        \n",
      "=================================================================\n",
      "Total params: 1,170,553\n",
      "Trainable params: 1,170,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0.946750697925\n",
      "0.925790592868\n",
      "[[2872  166]\n",
      " [ 316 1438]]\n"
     ]
    }
   ],
   "source": [
    " adam = Adam(lr=0.0001)\n",
    " sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    " model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    " print('running at most 60 epochs')\n",
    " checkpointer = ModelCheckpoint(filepath=\"HistoneMark_H3K9ac.hdf5\", verbose=1, save_best_only=True)\n",
    " earlystopper = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    " model.fit(X_train_H3K9ac, Y_train_H3K9ac, batch_size=128, epochs=50, shuffle=True, validation_data=( X_val_H3K9ac, Y_val_H3K9ac), callbacks=[checkpointer,earlystopper])\n",
    " #model.fit(X_train_s, Y_train_s, batch_size=12, epochs=50, shuffle=True, validation_data=( X_val_s, Y_val_s), callbacks=[checkpointer,earlystopper])\n",
    " y_pred = model.predict(X_test_H3K9ac)\n",
    " #y_pred = model.predict(X_test_s)\n",
    " #tresults = model.evaluate(X_test_s, Y_test_s)\n",
    " np.savetxt('H3K9ac_true.csv', Y_test_H3K9ac, delimiter=\",\")\n",
    " np.savetxt('H3K9ac_pred.csv', y_pred, delimiter=\",\")\n",
    " tresults = model.evaluate(X_test_H3K9ac, Y_test_H3K9ac)\n",
    " print(tresults)\n",
    " model.summary()\n",
    " #print(roc_auc_score(Y_test_s,y_pred))\n",
    " print(roc_auc_score(Y_test_H3K9ac, y_pred))\n",
    " print(average_precision_score(Y_test_H3K9ac, y_pred))\n",
    " y_pred = (y_pred>0.5)\n",
    " cm = confusion_matrix(Y_test_H3K9ac, y_pred)\n",
    " print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
